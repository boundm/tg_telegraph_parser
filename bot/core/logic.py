import aiohttp
import asyncio
import re
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

user_agent = UserAgent().random

class cs:
    INFO = '\033[93m'
    GREEN = '\033[92m'
    END = '\033[0m'

RU_EN_MAP = {
    "Ð°": "a", "Ð±": "b", "Ð²": "v", "Ð³": "g", "Ð´": "d", "Ðµ": "e", "Ñ‘": "yo",
    "Ð¶": "zh", "Ð·": "z", "Ð¸": "i", "Ð¹": "y", "Ðº": "k", "Ð»": "l", "Ð¼": "m",
    "Ð½": "n", "Ð¾": "o", "Ð¿": "p", "Ñ€": "r", "Ñ": "s", "Ñ‚": "t", "Ñƒ": "u",
    "Ñ„": "f", "Ñ…": "h", "Ñ†": "ts", "Ñ‡": "ch", "Ñˆ": "sh", "Ñ‰": "shch",
    "ÑŠ": "", "Ñ‹": "y", "ÑŒ": "", "Ñ": "e", "ÑŽ": "yu", "Ñ": "ya",
}

def slugify(text: str) -> str:
    text = text.lower()
    translit = "".join(RU_EN_MAP.get(c, c) for c in text)
    return re.sub(r"[^a-z0-9]+", "-", translit).strip("-")

async def fetch(session, url, sem):
    headers = {"User-Agent": user_agent}
    async with sem:
        try:
            async with session.get(url, headers=headers, timeout=5) as response:
                if response.status == 200 and 'html' in response.headers.get('Content-Type', ''):
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    date_tag = soup.find('time') or soup.find(class_='tl_article_date')
                    date_text = date_tag.text.strip() if date_tag else "â“ Ð”Ð°Ñ‚Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°"

                    print(f"{cs.GREEN}âœ… ÐÐ°Ð¹Ð´ÐµÐ½Ð¾: {url} â€” ðŸ—“ {date_text}{cs.END}")
                    return f"âœ… {url} â€” ðŸ—“ {date_text}"
                else:
                    print(f"{cs.INFO}âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: {url}{cs.END}")
        except Exception as e:
            print(f"{cs.INFO}âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {url} | {e}{cs.END}")
    return None

async def parse_title(title: str, offset: int = 2) -> list[str]:
    slug = slugify(title.strip())
    sem = asyncio.Semaphore(20)
    urls = []

    for month in range(1, 13):
        for day in range(31, 0, -1):
            for i in range(1, offset + 1):
                suffix = f"-{i}" if i > 1 else ""
                url = f"https://telegra.ph/{slug}-{month:02}-{day:02}{suffix}"
                urls.append(url)

    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url, sem) for url in urls]
        results = await asyncio.gather(*tasks)

    return [r for r in results if r is not None]
