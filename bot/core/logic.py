import aiohttp
import asyncio
import re
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from datetime import datetime

user_agent = UserAgent().random

class cs:
    INFO = '\033[93m'
    GREEN = '\033[92m'
    END = '\033[0m'

RU_EN_MAP = {
    "Ğ°": "a", "Ğ±": "b", "Ğ²": "v", "Ğ³": "g", "Ğ´": "d", "Ğµ": "e", "Ñ‘": "yo",
    "Ğ¶": "zh", "Ğ·": "z", "Ğ¸": "i", "Ğ¹": "y", "Ğº": "k", "Ğ»": "l", "Ğ¼": "m",
    "Ğ½": "n", "Ğ¾": "o", "Ğ¿": "p", "Ñ€": "r", "Ñ": "s", "Ñ‚": "t", "Ñƒ": "u",
    "Ñ„": "f", "Ñ…": "h", "Ñ†": "ts", "Ñ‡": "ch", "Ñˆ": "sh", "Ñ‰": "shch",
    "ÑŠ": "", "Ñ‹": "y", "ÑŒ": "", "Ñ": "e", "Ñ": "yu", "Ñ": "ya",
}

def slugify(text: str) -> str:
    text = text.lower()
    translit = "".join(RU_EN_MAP.get(c, c) for c in text)
    return re.sub(r"[^a-z0-9]+", "-", translit).strip("-")

# ğŸŸ¡ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° Ğ´Ğ°Ñ‚Ñ‹
def try_parse_date(text):
    for fmt in ("%B %d, %Y", "%d.%m.%Y", "%Y-%m-%d"):
        try:
            return datetime.strptime(text, fmt)
        except Exception:
            continue
    return None  # ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¾ÑÑŒ

async def fetch(session, url, sem):
    headers = {"User-Agent": user_agent}
    async with sem:
        try:
            async with session.get(url, headers=headers, timeout=5) as response:
                if response.status == 200 and 'html' in response.headers.get('Content-Type', ''):
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    date_tag = soup.find('time') or soup.find(class_='tl_article_date')
                    date_text = date_tag.text.strip() if date_tag else "â“ Ğ”Ğ°Ñ‚Ğ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°"

                    parsed_date = try_parse_date(date_text)
                    return {
                        "url": url,
                        "date_str": date_text,
                        "parsed_date": parsed_date
                    }
        except Exception as e:
            print(f"{cs.INFO}âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: {url} | {e}{cs.END}")
    return None

async def parse_title(title: str, offset: int = 2) -> list[str]:
    slug = slugify(title.strip())
    sem = asyncio.Semaphore(20)
    urls = []

    for month in range(1, 13):
        for day in range(31, 0, -1):
            for i in range(1, offset + 1):
                suffix = f"-{i}" if i > 1 else ""
                url = f"https://telegra.ph/{slug}-{month:02}-{day:02}{suffix}"
                urls.append(url)

    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url, sem) for url in urls]
        results = await asyncio.gather(*tasks)

    valid_results = [r for r in results if r is not None]

    # ğŸ”½ Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾ parsed_date Ğ¾Ñ‚ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğº ÑÑ‚Ğ°Ñ€Ñ‹Ğ¼
    sorted_results = sorted(valid_results, key=lambda r: r["parsed_date"] or datetime.min, reverse=True)

    # ğŸ“¤ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°
    return [f"âœ… {r['url']} â€” ğŸ—“ {r['date_str']}" for r in sorted_results]
